{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4: Extracting syntactic groups using machine-learning techniques: Prerequisites\n",
    "Author: Pierre Nugues\n",
    "\n",
    "__You must execute this notebook before your start the assignment.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the assignment is to create a system to extract syntactic groups from a text. You will apply it to the CoNLL 2000 dataset. \n",
    "\n",
    "In this part, you will collect the datasets and the files you need to train your models. You will also collect the script you need to evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting a Training and a Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As annotated data and annotation scheme, you will use the data available from [CoNLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/).\n",
    "1. Read the description of the CoNLL 2000 task\n",
    "2. Download both the training and test sets and decompress them. See below\n",
    "\n",
    "CoNLL 2000 is an early dataset and contrary to many current ones, it has no development set.\n",
    "\n",
    "You can also download them from this site: https://huggingface.co/datasets/conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\arvid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arvid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arvid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arvid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arvid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Arvid\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "\n",
    "download_file('http://www.clips.uantwerpen.be/conll2000/chunking/train.txt.gz', 'train.txt.gz')\n",
    "download_file('http://www.clips.uantwerpen.be/conll2000/chunking/test.txt.gz', 'test.txt.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def decompress_gz_file(input_file_path, output_file_path):\n",
    "    with gzip.open(input_file_path, 'rb') as f_in:\n",
    "        with open(output_file_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "decompress_gz_file('train.txt.gz', 'train.txt')\n",
    "decompress_gz_file('test.txt.gz', 'test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus/test.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directory\n",
    "os.makedirs('corpus', exist_ok=True)\n",
    "\n",
    "# Move files\n",
    "shutil.move('train.txt', 'corpus/train.txt')\n",
    "shutil.move('test.txt', 'corpus/test.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The evaluation script\n",
    "\n",
    "You will train the models with the training set and the test set to evaluate them. For this, you will apply the `conlleval` script that will compute the harmonic mean of the precision and recall: F1. \n",
    "\n",
    "`conlleval` was written in Perl. Some people rewrote it in Python and you will use such such a translation in this lab. The line below installs it. The source code is available from this address: https://github.com/kaniblu/conlleval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conlleval\n",
      "  Downloading conlleval-0.2-py3-none-any.whl (5.4 kB)\n",
      "Installing collected packages: conlleval\n",
      "Successfully installed conlleval-0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Arvid\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install conlleval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will represent the words with dense vectors, instead of a one-hot encoding. GloVe embeddings is one such representation. The Glove files contain a list of words, where each word is represented by a vector of a fixed dimension. In this notebook, we will use the file of 400,000 lowercase words with the 100-dimensional vectors.\n",
    "Download either:\n",
    "*  The GloVe embeddings 6B from <a href=\"https://nlp.stanford.edu/projects/glove/\">https://nlp.stanford.edu/projects/glove/</a> and keep the 100d vectors; or\n",
    "* A local copy of this dataset with the cell below (faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(\"https://fileadmin.cs.lth.se/nlp/nobackup/embeddings/nobackup/glove.6B.100d.txt.gz\", \"glove.6B.100d.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus/glove.6B.100d.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decompress_gz_file('glove.6B.100d.txt.gz', 'glove.6B.100d.txt')\n",
    "\n",
    "shutil.move('glove.6B.100d.txt', 'corpus/glove.6B.100d.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b092dd6cd17ec779f3bae5064f9d72da31268a5c4a5fd51a6f11e9df3477f045"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
